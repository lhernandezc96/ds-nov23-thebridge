{"cells":[{"cell_type":"markdown","metadata":{},"source":["![logo](./img/TheBridge_RL.png)"]},{"cell_type":"markdown","metadata":{},"source":["![ejercicio](./img/ejercicios.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# El lago congelado"]},{"cell_type":"markdown","metadata":{},"source":["## Contenidos"]},{"cell_type":"markdown","metadata":{},"source":["* [Imports iniciales y funciones de animacion](#Imports-iniciales-y-funciones-de-animacion)  \n","* [Aplicando Q-Learning](#Aplicando-Q-Learning)  \n","* [Evaluacion](#Evaluacion)  \n","* [EXTRA: Terrenos resbaladizos](#EXTRA:-Terrenos-resbaladizos)  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En el siguiente ejercicio vamos a intentar ayudar a un simpático muchacho a poder alcanzar un regalo navideño atravesando un lago helado con algún que otro punto peligroso donde el hielo están fino que de pisarlo se hundiría congelándose.\n","\n","En concreto, queremos que nuestro agente aprenda a cruzar el lago hasta el regalo en el escenario de la imagen:"]},{"cell_type":"markdown","metadata":{},"source":["<img src =\"./img/Frozen_Lake.png\" />"]},{"cell_type":"markdown","metadata":{},"source":["Para ello, vamos a utlizar uno de los entornos contenidos en el GYM de OpenAI que hemos aprendido a utilizar en las sesiones anteriores. Además emplearemos las funciones de animación de Matplotlib para poder evaluar visualmente el éxito o fracaso de nuestro aprendizaje por refuerzo."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Imports iniciales y funciones de animacion  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Como ayuda inicial, incluimos aquí los imports y las funciones básicas para hacer las animaciones, salvo una pequeña función que se pide implementar..."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":["import matplotlib.animation\n","import matplotlib.pyplot as plt\n","\n","plt.rc('font', size=14)\n","plt.rc('axes', labelsize=14, titlesize=14)\n","plt.rc('legend', fontsize=14)\n","plt.rc('xtick', labelsize=10)\n","plt.rc('ytick', labelsize=10)\n","plt.rc('animation', html='jshtml')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":["def update_scene(num, frames, patch):\n","    patch.set_data(frames[num])\n","    return patch,\n","\n","def plot_animation(frames, repeat=False, interval=40):\n","    fig = plt.figure()\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    anim = matplotlib.animation.FuncAnimation(\n","        fig, update_scene, fargs=(frames, patch),\n","        frames=len(frames), repeat=repeat, interval=interval)\n","    plt.close()\n","    return anim"]},{"cell_type":"markdown","metadata":{},"source":["Codifica ahora una función de nombre plot_environment que reciba un entorno como parámetro y pinte utilizando el metodo imshow del módulo pyplot, que hemos cargado anteriormente, para pintar el estado actual del entorno (tendrás que hacer uso del método render del entorno)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Importa el módulo gym e instancia en una variable el entorno \"FrozenLake-v1\". Utiliza también los parámetros, is_slippery = False y render_mode = \"rgb_array\". Recuerda terminar la llamada a la función make con \".env\" para saltarnos la limitación de duración de un episodio."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Reinicia el entorno, pinta el estado inicial así como el tamaño del espacio de acciones y el espacio de estados"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["La lista de acciones es:  \n","0: Mover a la izquierda\n","\n","1: Mover hacia abajo\n","\n","2: Mover a la derecha\n","\n","3: Mover hacia arriba"]},{"cell_type":"markdown","metadata":{},"source":["Compruébalo ejecutando la siguiente serie de movimientos (si hace falta resetea el entorno antes): [2,2,1,0,3].\n","Hazlo primero paso a paso utilizando la función plot_environment"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Y ahora a partir de la función plot_animation, recuerda que lo que se le pasa a esta función es un conjunto ordenado de imagenes (la que proporciona el método render al invocarlo env.render), tal como hicimos en el ejemplo del vagón y la montaña"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Aplicando Q-Learning  \n","[al indice](#Contenidos)  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Crea una estructura para la tabla Q (q-table)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Lo siguiente es seleccionar los valores de los hiperparámetros: la tasa de aprendizaje (alpha), el factor de descuento (gamma) y el factor epsilon, ya que implementaremos un epsilon-greedy como el que hemos visto en las sesiones anteriores.\n","\n","Prueba con los siguientes:  \n","alpha = 0.8  \n","gamma = 0.95  \n","epislon = 0.1  "]},{"cell_type":"code","execution_count":147,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Ahora es el momento de implementar la estimación de la tabla Q. Utilizando un algoritmo similar al empleado con el SmartCab pero con un número de episodios menor (prueba entre 500 y 2000), construye la tabla Q.\n","\n","Aunque no es necesario, puedes llevar un recuento de los episodios fallidos (nuestro querido agente cae al agua al pisar uno de los hoyos). Para ello ten en cuenta que el esquema de recompensas de este entorno es:\n","- Por llegar al regalo, 1 punto\n","- En cualquier otra situación, 0 puntos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Muestra la tabla-Q"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluacion  \n","[al indice](#Contenidos)  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ahora que tenemos nuestra tabla-Q estimada es hora de ver si realmente hemos tenido éxito con nuestro aprendizaje. \"Juega\" un episodio aplicando solo \"explotation\" (es decir escoge siempre la accion que de lugar al máximo Q) y visualizalo utilizando plot_animation.  \n","OJO: \n","Acuerdate de reinicializar el entorno y de guardar el env.render del estado inicial como primera imagen a pasar a plot_animation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["¿Qué tal ha ido? "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### EXTRA: Terrenos resbaladizos  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Como ejercicio adicional, repite el anterior pero esta vez creando utilizando la siguiente invocación del entorno"]},{"cell_type":"code","execution_count":133,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Al activar el modo \"resbaladizo\" (slippery), ahora las acciones no siempre tienen el efecto esperado, de hecho sólo una de cada tres veces intentar ir en una dirección nos llevará en esa dirección debido a lo resbaladizo del terreno.\n","\n","Repite el código de entrenamiento, con los mismos hiperparámetros que antes, y recuerda inicializar la tabla-Q!!! con ceros (créala de nuevo)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Muestra la nueva tabla de valores Q y compárala con la del entorno sin hielo resbaladizo"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ahora, como en el escenario anterior, escribe el código (puedes emplear el mismo que hiciste antes) para ejecutar un único episodio en el que la acción se escoja a partir de los valores de la tabla-Q. Al finalizar utiliza plot_animation para mostrar el episodio. De nuevo, recuerda reinicializar el entorno al principio y añadir como primer \"frame\" el resultado de env.render() tras la reinicialización."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":["\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ejecuta el episodio varias veces, hasta que ocurra algo inesperado... ¿Qué pasado? ¿Cómo lo explicas? "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":2}
