{"cells":[{"cell_type":"markdown","metadata":{},"source":["![logo](./img/TheBridge_RL.png)"]},{"cell_type":"markdown","metadata":{},"source":["# Taxi Autónomo (Smartcab), sin RL"]},{"cell_type":"markdown","metadata":{},"source":["## Contenidos"]},{"cell_type":"markdown","metadata":{},"source":["* [Definicion del Problema](#Definicion-del-Problema)  \n","* [Creando el entorno](#Creando-el-entorno)  \n","* [Sin aprendizaje por refuerzo](#Sin-aprendizaje-por-refuerzo)  \n","* [Visualizacion](#Visualizacion)  \n"]},{"cell_type":"markdown","metadata":{},"source":["### Definicion del Problema  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["__Hay 4 ubicaciones (etiquetadas con diferentes letras) y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra.__"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"./img/Reinforcement_Learning_Taxi_Env.png\" alt=\"drawing\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{},"source":["\n","Recuerda, que las recompensas y castigos estan fijadas en este entorno de la siguiente manera:__   \n","__* Recibimos +20 puntos por un traslado exitoso.__  \n","__* Perdemos 1 punto por cada intervalo de tiempo que tarda.__  \n","__* También hay una penalización de 10 puntos por acciones de recogida y dejada ilegales.__  \n","__* Un movimiento que de lugar a un \"choque\" tiene la penalización de perder tiempo (el taxi no se desplaza pero se recibe la penalización de duración)__"]},{"cell_type":"markdown","metadata":{},"source":["### Creando el entorno  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gym\n","\n","env = gym.make(\"Taxi-v3\",render_mode=\"ansi\").env\n","'''En esta línea, se crea un entorno de Gym llamado \"Taxi-v3\" y se almacena en la variable env. \n","El argumento render_mode=\"ansi\" se utiliza para configurar el modo de representación en el que se mostrará el entorno. \n","El \".env\" al final se utiliza para obtener el objeto del entorno real.'''\n","\n","state,info = env.reset(seed=19)\n","'''Aquí, se reinicia el entorno utilizando el método reset() y se proporciona una semilla (seed) igual a 19 como argumento.'''\n","\n","print(env.render())\n","'''Esta línea imprime el estado inicial del entorno utilizando el método render() del entorno Gym. \n","El método render() muestra la representación visual del entorno en el modo especificado (en este caso, \"ansi\").'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["movements = [2,0]\n","\n","for action in movements:\n","    env.step(action)\n","    print(env.render())"]},{"cell_type":"markdown","metadata":{},"source":["Tenemos 6 posibles acciones (sur, norte, este, oeste, recoger, dejar) y 500 posibles estados (que combinan la posición del taxi, el punto de recogida, el de entrega, la situación del pasajero) codificados del 0 al 499"]},{"cell_type":"markdown","metadata":{},"source":["### Sin aprendizaje por refuerzo  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"trusted":true},"outputs":[],"source":["timesteps = 0 \n","penalties, rewards = 0,0\n","simulated_time = 0\n","frames = []\n","\n","done = False #se utilizará para controlar cuando se detiene el bucle\n","while not done:\n","    action= env.action_space.sample()\n","    '''En cada paso del bucle, se elige una acción aleatoria utilizando el método sample() del espacio de acciones \n","    del entorno env. Esto simula la elección de una acción por parte de un agente de aprendizaje por refuerzo.'''\n","    if action < 4:\n","        simulated_time += 45\n","    else:\n","        simulated_time += 75\n","    '''Dependiendo de la acción elegida (menor que 4 o no), se incrementa el tiempo simulado simulated_time. \n","    Esto representa que algunas acciones consumen más tiempo que otras en la simulación.'''\n","    state,reward,done,truncated,info = env.step(action) #Se ejecuta la acción elegida en el entorno env utilizando el método step(action)\n","    if reward == -10:\n","        penalties += 1\n","    rewards += reward\n","    '''Se actualizan las estadísticas. Si la recompensa es igual a -10, se incrementa el contador de penalizaciones \n","    penalties, y se acumula la recompensa total rewards.'''\n","    frames.append({\n","        \"frame\":env.render(),\n","        \"state\": state,\n","        \"action\":action,\n","        \"reward\":reward,\n","        \"elapsed\":simulated_time  \n","    })\n","    '''Se registra información relevante en el paso actual en la lista frames. Esto incluye el fotograma (representación visual)\n","    del entorno, el estado, la acción, la recompensa y el tiempo simulado en ese momento.'''\n","    timesteps += 1\n","    '''Se incrementa el contador de timesteps para llevar un registro del número de pasos en la simulación.'''\n","\n","#UNA VEZ FINALIZA EL BUCLE, SE IMPRIMEN LAS ESTADÍSTICAS FINALES\n","print(\"Timesteps:\",timesteps)\n","print(\"Penalties:\",penalties)\n","print(\"Tiempo real simulado:\",simulated_time)"]},{"cell_type":"markdown","metadata":{},"source":["En tiempo de ejecución de código no ha sido nada... Pero imagina por un momento que hubiera sido una prueba en un entorno real... Recuerda que esta es una de las razones por las que se usan entornos simulados para aprender. Aunque no siempre es posible o recomendable (los coches autónomos de verdad aprenden sobre el terreno)"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizacion  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Para poder ver qué es lo que realmente está haciendo el \"smart\"cab, vamos a crearnos una función que pinte cada frame, dejando un intervalo de tiempo entre frame y frame para crear el efecto de animación."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# Importamos dos comandos necesarios para limpiar la pantalla y para generar el intervalo entre frames\n","from IPython.display import clear_output \n","from time import sleep\n","\n","def episode_animation(frames):\n","    for i, frame in enumerate(frames): # Recorremos todo el conjunto de frames\n","        clear_output(wait=True) # Limpiamos la \"pantalla\"\n","        print(frame['frame']) # Visualizamos el \"pantallazo\" resultado de cada acción\n","        print(f\"Timestep: {i + 1}\") # Aumentamos el contador de pasos/steps\n","        # Imprimimos el resto de valores correspondientes a cada frame y que hemos guardado al realizar el \"aprendizaje\"\n","        print(f\"State: {frame['state']}\") \n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        print(f\"Elapsed time (sec.): {frame['elapsed']}\")\n","        sleep(.1) # \"Dormimos\" el programa un tiempo para que nuestro ojo pueda ver la imagen antes de borrarla y mostrar la siguiente\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["episode_animation(frames[max(0,len(frames)-500):]) # Lanzamos la animacion, pero asegurándonos de que sólo mostramos como mucho 500 frames. Si quieres verla entera (para gustos los colores...), sólo tienes que quitar indexado y volver a ejecutar."]},{"cell_type":"markdown","metadata":{},"source":["Cómo puedes comprobar este mecanimo, aunque cumple con el cometido, tiene severas limitaciones:\n","- Los tiempos son inaceptables\n","- No se guarda el \"aprendizaje\", pero aunque lo hiciéramos, sólo valdría para la situación de partida (el taxi en la posición 3,1. Recoger en Y entregar en R)"]},{"cell_type":"markdown","metadata":{},"source":["Necesitamos un mecanismo de mejora de la estrategia que no sólo reduzca los tiempos y los optimice al máximo, sino que además sea flexible como para poder aplicarse en cualquier situación. Ese es el objetivo de la siguiente sesión: Q-Learning."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Ahora tú"]},{"cell_type":"markdown","metadata":{},"source":["Aunque a continuación tienes un ejercicio específico, ¿por qué no inventas una política más acertada para nuestro SmartCab? Por ejemplo puedes jugar con la variable info, ya que eso no es hacer trampa. Nuestro SmartCab tendría su LIDAR y otros sensores para detectar paredes y cuando se acaba el terreno por ejemplo. Es decir viendo la action_mask podría evitarse acciones \"inútiles\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
