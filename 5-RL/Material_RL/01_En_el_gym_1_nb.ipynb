{"cells":[{"cell_type":"markdown","metadata":{},"source":["![logo](./img/TheBridge_RL.png)"]},{"cell_type":"markdown","metadata":{},"source":["# Introducción a los entornos de prueba de aprendizaje por refuerzo (el Gym) (I)"]},{"cell_type":"markdown","metadata":{},"source":["## Contenidos"]},{"cell_type":"markdown","metadata":{},"source":["* [Motivacion](#Motivacion)  \n","* [Gym de OpenAI](#Gym-de-OpenAI)  \n","* [Primeros pasos con Gym](#Primeros-pasos-con-Gym)  \n","* [Steps, Episodes y Actions (pasos, episodios y acciones)](#Steps,-Episodes-y-Actions-(pasos,-episodios-y-acciones))  \n","* [Visualizacion (*Rendering*)](#Visualizacion-(*Rendering*))  \n"]},{"cell_type":"markdown","metadata":{},"source":["### Motivacion\n","  \n","[al indice](#Contenidos)  \n","Uno de los elementos principales de un sistema de aprendizaje por refuerzo es el entorno. Por tanto, tendremos que disponer de él.  \n","\n","Podría pensarse que cuanto más real, mejor, pero los entornos reales tienen también serias limitaciones.  \n","\n","Considera el caso de que queramos entrenar un robot de rescate en la montaña. Podemos pensar en entrenarlo en una montaña de verdad, pero:\n","* Si el robot se cae por una ladera no hay forma de hacer Ctrl+Z.\n","* Estamos limitados al tiempo real, es decir si queremos que el robot aprenda en un recorrido de 100Km, cada intento llevará el tiempo que le lleve al robot recorrer 100Km. No podemos acelerar el tiempo.\n","\n","Por eso, empleamos simuladores de entornos y por sus características, reglas y acciones concretas, que facilitan su simulación, se emplean videojuegos."]},{"cell_type":"markdown","metadata":{},"source":["### Gym de OpenAI  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Dentro de los posibles simuladores de entornos el más conocido es Gym(*) de OpenAI. Este simulador permite testar, desarrollar y comparar algoritmos de aprendizaje por refuerzo (desde los clásicos juegos de Atari a entornos \"reales\" en 2D y 3D, pasando por varios juegos de mesa).\n","\n","Uno de los puntos fuertes es que se emplea una interfaz única para todos los entornos, lo que permite escribir algoritmos genéricos. Sólo necesitaras cambiar una línea de código para probarlo en cualquiera de los entornos disponibles.   \n","\n","Comencemos!!!\n"]},{"cell_type":"markdown","metadata":{},"source":["### Primeros pasos con Gym  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Lo primero es importar el módulo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!pip install gym (minimo para seguir la clase)\n","#!pip install gym[atari] # para cargar los entornos de atari\n","#!pip install ale-py #Puede que necesites esto\n","#!pip install pygame #y esto\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["import gym"]},{"cell_type":"markdown","metadata":{},"source":["Para cargar un entorno sólo tienes que teclear lo siguiente, indicando el nombre del entorno que se quiera simular"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env=gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{},"source":["En este caso estamos instanciando un entorno de un juego cuyo objetivo es mover un vagón en el fondo de un valle de forma que alcance la parte superior de una colina."]},{"cell_type":"markdown","metadata":{},"source":["<img src= \"./img/mountain_car_example.png\" width = 500 />"]},{"cell_type":"markdown","metadata":{},"source":["Si queremos ver todos los entornos disponibles sólo tenemos que tecelar lo siguiente"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["envs = gym.envs.registry\n","sorted(envs.keys())"]},{"cell_type":"markdown","metadata":{},"source":["Una vez cargado un entorno, lo primero que hacemos es inicializarlo:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env.reset()"]},{"cell_type":"markdown","metadata":{},"source":["Y nos devuelve un array con las observaciones, e información adicional. \n","En este entorno, las observaciones son la posición en el eje-x, y la velocidad del vagón."]},{"cell_type":"markdown","metadata":{},"source":["### Steps, Episodes y Actions (pasos, episodios y acciones)  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["En el contexto del aprendizaje por refuerzo, nos quedan dos conceptos por conocer:\n","- Step (Paso): La ejecución de una acción de un agente sobre un entorno que hace que este devuelva las observaciones del nuevo estado y la recompensa\n","- Episode (Episodio): Conjunto completo de pasos (steps) que conforman un ciclo completo de entrenamiento, lo que viene a ser una \"partida\""]},{"cell_type":"markdown","metadata":{},"source":["En el caso de Gym, la forma de ejecutar un step, es la siguiente:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env.step(action) #no ejecutar la celda"]},{"cell_type":"markdown","metadata":{},"source":["¿Y qué acciones?"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env.action_space"]},{"cell_type":"markdown","metadata":{},"source":["Nos dice que hay tres acciones, discretas, es decir, accion 0, acción 1, y acción 2\n","Si consultamos la documentación de OpenAI Gym:\n","  \n","0: Acelerar a la izquierda  \n","1: No hacer nada  \n","2: Acelerar a la derecha  "]},{"cell_type":"markdown","metadata":{},"source":["Si queremos más información, en la misma documentación podemos ver que fórmula sigue el entorno para simular la aceleración o, importante cual es el criterio de recompensa (en este caso castigo):\n","- Como el objetivo es llegar a la cima lo antes posible, a cada acción le corresponde siempre una recompensa de -1. Este es un caso en el que querremos minimizar el reward de cada episodio."]},{"cell_type":"markdown","metadata":{},"source":["Ahora que conocemos las acciones, ejecutemos un step como si fueramos el agente:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env.step(0)"]},{"cell_type":"markdown","metadata":{},"source":["Nos ha devuelto cinco valores:\n","* __las observaciones__ del nuevo estado, es decir la posición en el eje-x y la velocidad con su signo (como hemos indicado que acelere a la izquierda y estaba quieto ha cogido velocidad \"negativa\", hacia la izquierda)\n","* __el reward__ que como hemos comentado siempre es -1\n","* __un flag de terminated__ nos indica si el episodio se ha terminado porque se ha alcanzado alguna de las condiciones de parada (en nuestro caso si se ha llegado a la cima)\n","* __un flag de truncado__ es un flag especial por si hay problemas con el entorno y debe reinicializarse\n","* __información adicional__ que se emplea en algunos entornos, como por ejemplo los juegos de Atari para indicarnos las vidas que le quedan al agente"]},{"cell_type":"markdown","metadata":{},"source":["Típicamente, ejecutaremos el step almacenando su salida en diferentes variables, algo como:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["obs,reward,done,truncated,info = env.step(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(obs)"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizacion (*Rendering*)  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Con los comandos vistos ya podríamos programar un ciclo completo de entrenamiento de aprendizaje por refuerzo, pero de una forma bastante árida y con poca capacidad por parte de un ser humano de poder \"ver\" cómo está evolucionando."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env.render()"]},{"cell_type":"markdown","metadata":{},"source":["Lo que nos ha devuelto es el contenido de la pantalla pixel a pixel que corresponde al estado generado por nuestra acción. Esto es así porque cuando creamos el entorno utilizamos el argumento render_mode con el valor \"rgb_array\":   \n","```python\n","env = gym.make(\"MountainCar-v0\", render_mode = \"rgb_array\")\n","```"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a emplear un método que utiliza una librería como matplotlib que funciona con cualquier sistema operativo"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","img=env.render()\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["Ejecutemos varios steps seguidos acelerando hacia la izquierda y veamos el resultado final"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range (100):\n","    obs,reward,done,truncated,info = env.step(0)\n","    \n","img = env.render()\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["**INSTALACION**: Gym como tal dejó de ser evolucionado por OpenAI a finales de 2022, pero todavía está operativo. La alternativa es gymnasium. Te dejo aquí los links a ambos y ojo mira la documentación por si te lo quieres instalar con diferentes entornos a los que vienen con la configuración básica (la de hacer pip install gym, o pip install gymnasium)\n","\n","[Al Gym](https://www.gymlibrary.dev/index.html)  \n","\n","[Al Gymnasium](https://gymnasium.farama.org/)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":2}
