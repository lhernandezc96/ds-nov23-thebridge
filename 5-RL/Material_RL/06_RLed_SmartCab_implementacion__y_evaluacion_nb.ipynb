{"cells":[{"cell_type":"markdown","metadata":{},"source":["![logo](./img/TheBridge_RL.png)"]},{"cell_type":"markdown","metadata":{},"source":["# Taxi Autónomo (Smartcab), CON RL\n"]},{"cell_type":"markdown","metadata":{},"source":["## Contenidos"]},{"cell_type":"markdown","metadata":{},"source":["* [Inicializacion](#Inicializacion)  \n","* [Q-Learning](#Q-Learning)  \n","* [A programar...](#A-programar...)  \n"]},{"cell_type":"markdown","metadata":{},"source":["### Inicializacion  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gym\n","import warnings\n","\n","\n","env = gym.make(\"Taxi-v3\", render_mode = \"ansi\").env"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env.reset(seed = 19)\n","print(env.render())\n","print(\"Current State:\", env.s)\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["movements = [2,0]\n","for mov in movements:\n","    env.step(mov)\n","    print(env.render())\n","    print(\"State:\",env.s)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from time import sleep\n","from IPython.display import clear_output\n","\n","def episode_animation(frames):\n","    for i, frame in enumerate(frames): # Recorremos todo el conjunto de frames\n","        clear_output(wait=True) # Limpiamos la \"pantalla\"\n","        print(frame['frame']) # Visualizamos el \"pantallazo\" resultado de cada acción\n","        print(f\"Timestep: {i + 1}\") # Aumentamos el contador de pasos/steps\n","        # Imprimimos el resto de valores correspondientes a cada frame y que hemos guardado al realizar el \"aprendizaje\"\n","        print(f\"State: {frame['state']}\") \n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        print(f\"Elapsed time (sec.): {frame['elapsed']}\")\n","        sleep(.1) # \"Dormimos\" el programa un tiempo para que nuestro ojo pueda ver la imagen antes de borrarla y mostrar la siguiente"]},{"cell_type":"markdown","metadata":{},"source":["### Q-Learning  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Recordemos brevemente los pasos del algoritmo de Q-Learning epsilon-greedy que nos permitirá estimar la Q-table"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"./img/q-matrix-initialized-to-learned_gQq0BFs.png\" alt=\"drawing\" width=\"650\"/>"]},{"cell_type":"markdown","metadata":{},"source":["Desglosándolo en pasos, obtenemos:\n","\n","* Inicializar la tabla Q con todos ceros.\n","* Seleccionar los valores de los hiperparámetros\n","* Comenzar a explorar acciones: Para cada estado, seleccione cualquiera entre todas las acciones posibles para el estado actual (S).\n","* Viajar al siguiente estado (S') como resultado de esa acción (a).\n","* Para todas las acciones posibles desde el estado (S') seleccione la que tenga el valor Q más alto.\n","* Actualizar los valores de la tabla Q usando la ecuación ya vista.\n","* Establecer el siguiente estado como el estado actual.\n","* Si se alcanza el estado objetivo, entonces terminar y repetir el proceso.\n"]},{"cell_type":"markdown","metadata":{},"source":["### A programar...  \n","[al indice](#Contenidos)  \n"]},{"cell_type":"markdown","metadata":{},"source":["Lo primero creamos la estructura de datos que nos permita almacenar la Q-table"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","q_table"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["q_table.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["q_table.size"]},{"cell_type":"markdown","metadata":{},"source":["Lo siguiente es seleccionar los valores de los hiperparámetros, alpha, gamma y épsilon"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["alpha = 0.05 #aprendizaje relativamente lento\n","gamma = 0.9 #darle prioridad a recompensas futuras\n","epsilon = 0.1 #establecer un 10% de acciones como aleatorias"]},{"cell_type":"markdown","metadata":{},"source":["Ahora podemos crear el algoritmo de entrenamiento que actualizará esta tabla Q a medida que el agente explore el entorno a lo largo de miles de episodios.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","import random\n","\n","all_epochs=[]\n","all_penalties=[]\n","\n","num_episodes = 100000\n","\n","state = env.s\n","\n","for i in range(1,num_episodes+1):\n","    epochs,penalties,reward = 0,0,0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0,1) < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            action = np.argmax(q_table[state])\n","        next_state,reward,done,truncated,info = env.step(action)\n","        \n","        next_max=np.max(q_table[next_state]) #maxQ(S',a')\n","        old_value = q_table[state,action]\n","        \n","        new_value = (1-alpha) * old_value + alpha * (reward + gamma * next_max)\n","        \n","        q_table[state,action] = new_value\n","        \n","        if reward == -10:\n","            penalties += 1\n","        \n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output (wait=True)\n","        print(f\"Episode: {i},{i/num_episodes * 100:.2f}\")\n","\n","    state,info = env.reset()\n","print(\"Entrenamiento finalizado\")"]},{"cell_type":"markdown","metadata":{},"source":["Ahora que hemos estimado la tabla Q tras los 100,000 episodios, veamos cuáles son los valores Q en el estado de nuestra ilustración, que recordemos es el correspondiente al índice 328\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["q_table[328]"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"trusted":true},"outputs":[],"source":["q_table"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluación"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a evaluar el rendimiento de nuestro agente. Ya no necesitamos explorar acciones, así que ahora la siguiente acción siempre se selecciona usando el mejor valor Q:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","total_epochs,total_penalties,total_reward = 0,0,0\n","num_episodes = 100\n","state = env.s\n","\n","set_frames = [] #Tendrá un elemento por episodio que contendrá los frames de ese episodio\n","#e información adicional. \n","\n","for i in range(1, num_episodes +1): \n","    epochs, penalties, reward = 0,0,0\n","    done = False\n","    frames = []\n","    \n","    while not done: \n","    \n","        action = np.argmax(q_table[state])\n","        state, reward, done, truncated, info = env.step(action)\n","        \n","        total_reward += reward\n","        frames.append({\n","            \"frame\": env.render(),\n","            \"state\":state,\n","            \"action\":action,\n","            \"reward\":reward,\n","            \"elapsed\": 0\n","        })\n","        \n","        if reward == -10:\n","            penalties += 1\n","            \n","        \n","        epochs += 1\n","    set_frames.append(frames)\n","    total_epochs += epochs\n","    total_penalties += penalties\n","    state,info = env.reset()\n","    \n","print(f\"Resultados después de {num_episodes} episodios\")\n","print(f\"Numero medio de acciones por episodio: {total_epochs/num_episodes}\")\n","print(f\"Numero medio de penalizaciones por episodio: {total_penalties/num_episodes}\")\n","print(f\"Recompensa media  por episodio: {total_reward/num_episodes}\")"]},{"cell_type":"markdown","metadata":{},"source":["Recuperamos la función de visualización para ver algunos \"episodios\"."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from time import sleep\n","from IPython.display import clear_output\n","\n","def episode_animation(frames):\n","    for i, frame in enumerate(frames): # Recorremos todo el conjunto de frames\n","        clear_output(wait=True) # Limpiamos la \"pantalla\"\n","        print(frame['frame']) # Visualizamos el \"pantallazo\" resultado de cada acción\n","        print(f\"Timestep: {i + 1}\") # Aumentamos el contador de pasos/steps\n","        # Imprimimos el resto de valores correspondientes a cada frame y que hemos guardado al realizar el \"aprendizaje\"\n","        print(f\"State: {frame['state']}\") \n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        print(f\"Elapsed time (sec.): {frame['elapsed']}\")\n","        sleep(.1) # \"Dormimos\" el programa un tiempo para que nuestro ojo pueda ver la imagen antes de borrarla y mostrar la siguiente"]},{"cell_type":"markdown","metadata":{},"source":["Utilicemos ahora la visualización para ver cuanto de bien ha aprendido a conducir. Vamos a analizar 5 episodios escogidos aleatoriamente."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from random import sample \n","from time import sleep \n","\n","for frame in sample (set_frames,5):\n","    episode_animation(frame[0:1])\n","    sleep(3)\n","    episode_animation(frame[1:])\n","    sleep(1)"]},{"cell_type":"markdown","metadata":{},"source":["Bastante bien, ¿no? Comparemos ahora con el \"entrenamiento\", por llamarlo de alguna forma, sin aprendizaje por refuerzo"]},{"cell_type":"markdown","metadata":{},"source":["### Comparando nuestro agente de Q-learning con no usar Aprendizaje por Refuerzo\n","  \n","\n","[al indice](#Contenidos)  \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a evaluar a nuestros agentes de acuerdo con las siguientes métricas,\n","\n","* Número promedio de penalizaciones por episodio: Cuanto menor sea el número, mejor será el rendimiento de nuestro agente. Idealmente, nos gustaría que esta métrica sea cero o muy cercana a cero.\n","* Número promedio de pasos por episodio: También queremos que sea un valor pequeño, que nuestro agente tome la ruta más corta para llegar al destino.\n","* Recompensas promedio por movimiento: Una recompensa más grande significa que el agente está haciendo lo correcto. Es por eso que decidir las recompensas es una parte crucial del Aprendizaje por Refuerzo.\n"]},{"cell_type":"markdown","metadata":{},"source":["Recuperemos el código que ya desarrollamos en la sesión sin aprendizaje por refuerzo para obtener los valores anteriores para este escenario y hacer la comparativa"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"Evaluate agent's performance without Q-learning\"\"\"\n","\n","total_epochs, total_penalties, total_rewards = 0, 0, 0\n","episodes = 100\n","\n","for _ in range(episodes):\n","    env.reset()\n","    # Crea el estado inicial\n","    state = env.encode(3, 1, 2, 0)\n","    env.s = state\n","    # Inicializa las epochs, penalties y rewards\n","    epochs, penalties, reward = 0, 0, 0\n","\n","    done = False\n","    actions = []\n","    while not done:\n","        # Elige la acción random\n","        action = env.action_space.sample()\n","        actions.append(action)\n","        # Ejecuta la accion\n","        state, reward, done, truncated, info = env.step(action)\n","        total_rewards += reward\n","        # Actualiza el valor de penalties si el reward es -10\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","print(f\"Average reward per step: {total_rewards/total_epochs}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":2}
